\documentclass{article}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{indentfirst}   % Indents first paragraph. change if u want ig
\usepackage{setspace}
\doublespacing

\begin{document}
\title{\textbf{Dynamic Programming: Bottum-up Problem Solving}}
\author{Remington Greko, Tyler Gutowski, and Spencer Hirsch}
\date{\today}

\maketitle

\noindent \textbf{Bottom-up Problem Solving} 


\noindent Richard Bellman described this problem solving technique in the
1950’s and called it Dynamic Programming to impress his sponsors.
Dynamic Programming is an important problem solving paradigm.
\textbf{Read about Bellman on Wikipedia and summarize the importance
of his contributions.}

\noindent \textit{Bellman Summary:}

Richard Bellman was an applied mathematician who was the first to 
introduce the concept of dynamic programming to the field. One of
his first major contributions was the Bellman Equation. This equation 
is also known as dynamic programming and outlines how to optimize
the solution of a given problem. Uses of this equation were first 
introduced to the engineering and mathematics fields, and later
made its way into the economic field as well.

A second major contribution Bellman made was the Hamilton-Jacobi-Bellman
Equation. It is a partial differential equation which is related to his
previously created Bellman Equation. This new equation provides the 
"optimal cost-to-go for a given dynamical system" (Bellman Wikipedia).
This equation is closesly related to the Bellman equation and was another
pioneering push for the development of the theory of dynamic programming.

A third major contribution is the Curse of Dimensionality which describes 
the issues with the "exponential increase in volume" (Bellman Wikipedia)
when adding dimensions to a space. The curse demonstrated how some 
problems may grow exponentially when introducing "more state variables 
to the value function" (Bellman Wikipedia). This means that the volume
of the mathematicl space increases so fast that the data becomes sparse
and therefore requires much more data to obtain a result (Curse of
Dimensionality Wikipedia).

The final major contribution discussed on Bellman's Wikipedia page is 
the Bellman-Ford algorithm, also known as the Label Correcting Algorithm.
This algorithm computes "single-source shortest paths" (Bellman Wikipedia)
in weighted digraphs where some edges may have negative values. This 
algorithm is very similar to Dijkstra's algorithm but, unlike Dijskta's, 
works with negative edge values.

\bigskip

\noindent \textbf{Describe how dynamic programming works: Solve and memoriz-
ing solutions to small problems and use these solutions as building
blocks to construct a solution to a larger problem from the bottom-
up.}

\medskip

Dynamic programming is used to optimize solutions of problems that can be solved recursively.
This is beneficial because recursion can lead to many different issues because it can be an 
expensive algorithm to implement. The concept of memorization is relatively simple, you break up a large problem
into subproblems where the results are stored to ensure that they are not computed again.
This will reduce the time complexity of an application making it a much more optimal solution
to many problems.

There are two primary approaches to dynamic programming, top-down and bottom-up. They both utilize
breaking a problem into smaller subproblems, however they have different execution techniques.
In top-down you will work your way through the subproblems just as they would be laid out in a tree.
Meaning that there are different paths a program can take depending on the problem. Whereas, with
bottom-up programming the layout of the subproblems is fixed and the program will only work through
subproblems that are strictly necessary.

Memorization is used when the result of a subproblem is calculated so that it will not have
to be recalculated, this is especially important with dynamic programming because with recursion
the same problem occurs more than once. The smaller subproblems all contribute to the overall 
problem, rather than having an algorithm working to solve every portion of the problem at a 
single time.

The bottom-up approach to dynamic programming ensures that the order of the subproblems will
garuntee that a subproblem will only need to be computed once. This will allow a program to
consume less memory. 

Both of the primary dynamic programming techniques are optimzed versions of recursion that
break down a larger problem into smaller subproblems, ensuring that a problem will only be solved
when it is necessary. Implementing memorization for any technique will ensure that a solution is
saved and will not have to be recalculated bring down the time complexity of the problem as
opposed to a recursive solution.



\bigskip

\noindent \textit{Example uses of Dynamic Programming}


\textbf{Give at least three examples problems that use the dynamic program-
ming paradigm to solve problems. (You should be able to search
finding examples and summarize these in your group’s words.)}

\bigskip

\noindent \textbf{Example One: (Spencer)}

\noindent \textit{Coin Change Problem:}

One problem that can utilize dynamic programming to optimize the
solution is the coin change problem. I chose this problem because 
it reminds me of a problem that we had in CSE 1002 and is something
that I often think about because I typically handle cash at work.\\

\noindent \textit{The problem goes as follows:}

Given an unlimited supply of coins as well as the denominations of
coins as input. Find all possible ways the desired change, also given 
as input, can be returned. There are always different solutions to
returning change, however, we typically default to the most convient,
least number of coins.\\

This problem can he solved using recursion, which would most likely be
the go-to solution for most programmers. However, using dynamic programming
you can optimize this solution bringing down the time, space and memory
complexity of the problem. This being said, using dynamic programming would
be the best way to solve this particular problem.

\bigskip

\noindent \textbf{Example Two: (Remi)}

\noindent \textit{Longest Common Subsequence:}

A second example of a problem which can be solved using dynamic programming 
is the longest common subsequence problem. This problem is one that I 
remember learning in data structures $\&$ algorithms, so I chose it for my 
example.\\

\noindent \textit{The problem goes as follows:}

Given two strings, find the longest subsequence of characters that is 
shared between the two strings. The dynamic approach to solving this problem
is performed in O(x * y) time where x and y are the lengths of the two 
strings being compared. This is a far better complexity than an approach 
where every possible subsequence is generated for each string and then 
compared. This would be an example of a brute-force approach, and it's time 
complexity is far worse.

\bigskip

\noindent \textbf{Example Three: ()}




\pagebreak

\noindent \textit{Good approximations to hard problems}

\medskip

There are Hard Problems: Problems that do not (seem) to have effi-
cient solutions. You will explore some of these later in future quests.

\medskip

There are dynamic programming algorithms that provide good ap-
proximate solutions to these hard problems. My memory tells me the
0–1 Knapsack Problem is an example of a hard problem with a good
dynamic programming approximation. \textbf{Report on this example and
at least 2 additional hard problems with good approximate solutions.}

\bigskip

\noindent \textbf{0-1 Knapsack Problem: (Spencer)} 

\noindent \textit{The problem goes as follows:} \\

The 0-1 Knapsack problem is a programming problem where as input you
are given, an array of items, [1, 2, ...., n - 1, n], and their 
respective weights given as an array, where each index in the array
corresponds to it's respective item. You are also given a maximum capacity
of the the sack that is going to be used to hold the items. You must find
the maximum number of items you can fit into the knapsack without exceeding
the capacity of the knapsack. However, the quantity of the items that can
be put into the bag is either zero or one of each specific item.

\medskip

\noindent \textit{The Solution:} \\

Using the iterative dynamic programming approach you would define a 2d array,
where the rows corresponds to the index of the items and the weights are defined
on the columns. For every weight you can either choose to ignore it or use it
when constructing the 2d array. Thus, you can calculate the maximum weight
that can be held in the knapsack based on the items and their corresponding
weights.

By iterating through the 2d array and constructing all possibilites, comparing
the maximum for each to the absolute maximum of the knapsack will give you 
the best solution to the given problem. The use of dynamic programming gives
the most optimal space and time complexity as opposed to the brute force
approach that could also be used for this problem.

\bigskip

\noindent \textbf{Longest Common subsequence: (Remi)} 

\noindent \textit{The problem goes as follows:} \\

The shortest path problem is a programming problem where the goal is to find 
the quickest route between an origin and goal point. In programming the points
travelled would be represented by a graph where, for example, cities are denoted
by nodes and roads are denoted by edges. Each edge is assigned a weight which 
represents the length of the road between any two nodes. The combination of edges 
to arrive at the goal with the lowest total value represents the shortest path.

\medskip

\noindent \textit{The Solution:} \\

One very popular algorithm used to solve this problem is Dijkstra's algorithm, 
which utilizes a dynamic approach in a graph. Firstly the start node is assigned 
a distance of 0 and every other node is assigned infinity. First the source node
is added to the queue. Each step pops the node with the shortest distance to the
source node and checks the distance of all of its neighboring nodes. If the 
new distance is shorter than the previously assigned one, it is updated. The 
neighboring nodes are then added to the queue and the process repeats until 
all nodes have either been assigned a value, or the goal node has been reached.

Dijkstra's algoirithm garuntees the optimal path between any two nodes, meaning 
that it will not waste time producing paths which are not the shortest between
the two input nodes. By updating the shortest path to every node at every step, 
the algorithm will never have to backtrack and reevaluate a distance which
has alreaddy been assigned. No unnecessary calculations are perfomed, meaning 
the time and space complexity is satisfactory.


\pagebreak

\begin{center}
        \begin{tabular}{|p{3cm}|p{6cm}|}
            \hline
            \textbf{Name} & \textbf{Section} \\
            \hline
            Remington Greko & Second example of Dynamic Programming $\&$ Shortest Path\\
            \hline
            Tyler Gutowski &  \\
            \hline
            Spencer Hirsch & How Dynamic Programming Works, One Example use of Dynamic Programming $\&$ 0-1 Knapsack Problem algorithm example \\
            \hline
        \end{tabular}
    \end{center}
    

\end{document}